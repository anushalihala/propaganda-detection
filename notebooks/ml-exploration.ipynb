{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iWgphS6DXigR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from  sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.modelsearch import *\n",
    "from utils.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K_AUlqT1gB0w"
   },
   "source": [
    "# Load Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uYj0qGJRWKB6"
   },
   "outputs": [],
   "source": [
    "train_file_name = '..//data//raw//train//task1.train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5348,
     "status": "ok",
     "timestamp": 1548595710776,
     "user": {
      "displayName": "Anusha Lihala",
      "photoUrl": "",
      "userId": "03154699776691367912"
     },
     "user_tz": -330
    },
    "id": "_k8ihpHEWJIi",
    "outputId": "44a58bbd-a67b-422e-cd1b-d9d95b103dda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the training set: 35993\n"
     ]
    }
   ],
   "source": [
    "articles_id, articles_content, gold_labels = ([], [], [])\n",
    "with open(train_file_name, \"r\", encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        article_content, article_id, gold_label = line.rstrip().split(\"\\t\")\n",
    "        articles_id.append(article_id)\n",
    "        articles_content.append(article_content)\n",
    "        gold_labels.append(gold_label)\n",
    "print(\"Number of documents in the training set: %d\"%(len(articles_content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame({'id':articles_id, 'text': articles_content, 'target': gold_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35993, 3)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>727600136</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Et tu, Rhody?  A recent editorial in the Provi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>731714618</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>A recent post in The Farmington Mirror — our t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>731714635</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>President Donald Trump, as he often does while...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>728627182</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>February is Black History Month, and nothing l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>728627443</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>The snow was so heavy, whipped up by gusting w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id          target  \\\n",
       "0  727600136  non-propaganda   \n",
       "1  731714618  non-propaganda   \n",
       "2  731714635  non-propaganda   \n",
       "3  728627182  non-propaganda   \n",
       "4  728627443  non-propaganda   \n",
       "\n",
       "                                                text  \n",
       "0  Et tu, Rhody?  A recent editorial in the Provi...  \n",
       "1  A recent post in The Farmington Mirror — our t...  \n",
       "2  President Donald Trump, as he often does while...  \n",
       "3  February is Black History Month, and nothing l...  \n",
       "4  The snow was so heavy, whipped up by gusting w...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.Series(train['target'].map({'propaganda':1,'non-propaganda':0}))\n",
    "train = train.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>727600136</td>\n",
       "      <td>Et tu, Rhody?  A recent editorial in the Provi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>731714618</td>\n",
       "      <td>A recent post in The Farmington Mirror — our t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>731714635</td>\n",
       "      <td>President Donald Trump, as he often does while...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>728627182</td>\n",
       "      <td>February is Black History Month, and nothing l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>728627443</td>\n",
       "      <td>The snow was so heavy, whipped up by gusting w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text\n",
       "0  727600136  Et tu, Rhody?  A recent editorial in the Provi...\n",
       "1  731714618  A recent post in The Farmington Mirror — our t...\n",
       "2  731714635  President Donald Trump, as he often does while...\n",
       "3  728627182  February is Black History Month, and nothing l...\n",
       "4  728627443  The snow was so heavy, whipped up by gusting w..."
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of preprocessing steps;\n",
    "\n",
    "* add vader sentiments\n",
    "* tokenize words\n",
    "* convert to lowercase\n",
    "* replace links with tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defined in preprocessing.py in utils\n",
    "sentiment_features_dict = get_sentiment_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save features\n",
    "with open('..//data//processed//sentiment_features.pickle', 'wb') as handle:\n",
    "    pickle.dump(sentiment_features_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load features\n",
    "with open('..//data//processed//sentiment_features.pickle', 'rb') as handle:\n",
    "    sentiment_features_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(pd.DataFrame(sentiment_features_dict), on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>neg_max</th>\n",
       "      <th>neg_median</th>\n",
       "      <th>neg_min</th>\n",
       "      <th>neu_max</th>\n",
       "      <th>neu_median</th>\n",
       "      <th>neu_min</th>\n",
       "      <th>pos_max</th>\n",
       "      <th>pos_median</th>\n",
       "      <th>pos_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>727600136</td>\n",
       "      <td>Et tu, Rhody?  A recent editorial in the Provi...</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8220</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>731714618</td>\n",
       "      <td>A recent post in The Farmington Mirror — our t...</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8960</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>731714635</td>\n",
       "      <td>President Donald Trump, as he often does while...</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8850</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>728627182</td>\n",
       "      <td>February is Black History Month, and nothing l...</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9635</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>728627443</td>\n",
       "      <td>The snow was so heavy, whipped up by gusting w...</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8960</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  neg_max  \\\n",
       "0  727600136  Et tu, Rhody?  A recent editorial in the Provi...    0.420   \n",
       "1  731714618  A recent post in The Farmington Mirror — our t...    0.565   \n",
       "2  731714635  President Donald Trump, as he often does while...    0.672   \n",
       "3  728627182  February is Black History Month, and nothing l...    0.490   \n",
       "4  728627443  The snow was so heavy, whipped up by gusting w...    0.500   \n",
       "\n",
       "   neg_median  neg_min  neu_max  neu_median  neu_min  pos_max  pos_median  \\\n",
       "0         0.0      0.0      1.0      0.8220    0.471    0.444       0.087   \n",
       "1         0.0      0.0      1.0      0.8960    0.374    0.626       0.000   \n",
       "2         0.0      0.0      1.0      0.8850    0.328    0.375       0.000   \n",
       "3         0.0      0.0      1.0      0.9635    0.510    0.487       0.000   \n",
       "4         0.0      0.0      1.0      0.8960    0.000    0.277       0.000   \n",
       "\n",
       "   pos_min  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    final_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            final_tokens.append('URL')\n",
    "        else:\n",
    "            final_tokens.append(token.lower_)\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LrJaNUqjn33"
   },
   "source": [
    "Split into Train and Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train.values, target.values, test_size=0.2, random_state=RANDOM_STATE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UYbKWP2XhFJ_"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 1), stop_words='english', lowercase=False, tokenizer=tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 139567,
     "status": "ok",
     "timestamp": 1548598316747,
     "user": {
      "displayName": "Anusha Lihala",
      "photoUrl": "",
      "userId": "03154699776691367912"
     },
     "user_tz": -330
    },
    "id": "WHTWQX4yX5td",
    "outputId": "0e71d34b-5b73-4c1f-9aac-590a5ea2e0b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking that the number of features in train and dev correspond: 186428 - 186428\n"
     ]
    }
   ],
   "source": [
    "text_train = vectorizer.fit_transform(X_train[:,1])\n",
    "text_valid = vectorizer.transform(X_valid[:,1])\n",
    "print(\"Checking that the number of features in train and dev correspond: %s - %s\" % (text_train.shape[1], text_valid.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28794, 186437)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_matrix = hstack([text_train, X_train[:, 2:].astype(float)])\n",
    "train_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7199, 186437)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_matrix = hstack([text_valid, X_valid[:, 2:].astype(float)])\n",
    "valid_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {'accuracy':lambda y,y_pred: accuracy_score(y, y_pred),\n",
    "                'f1':lambda y,y_pred: f1_score(y, y_pred, labels=[0,1])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "del get_metrics,print_metrics,model_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [('Logistic Regression', '', LogisticRegression()), \\\n",
    "              ('Naive Bayes', '', MultinomialNB()), \\\n",
    "              ('SVM', '', SVC()), \\\n",
    "              ('Decision Trees', '', DecisionTreeClassifier()), \\\n",
    "              ('Random Forest', '', RandomForestClassifier()), \\\n",
    "              ('AdaBoost', '', AdaBoostClassifier())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Logistic Regression ...\n",
      "Training scores:\n",
      "accuracy = 0.9548864346738903\n",
      "f1 = 0.7585950566809143\n",
      "Validation scores:\n",
      "accuracy = 0.9441589109598555\n",
      "f1 = 0.6959152798789712\n",
      "\n",
      "Fitting Naive Bayes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores:\n",
      "accuracy = 0.8881364173091616\n",
      "f1 = 0.0\n",
      "Validation scores:\n",
      "accuracy = 0.8888734546464787\n",
      "f1 = 0.0\n",
      "\n",
      "Fitting SVM ...\n",
      "Training scores:\n",
      "accuracy = 0.8881364173091616\n",
      "f1 = 0.0\n",
      "Validation scores:\n",
      "accuracy = 0.8888734546464787\n",
      "f1 = 0.0\n",
      "\n",
      "Fitting Decision Trees ...\n",
      "Training scores:\n",
      "accuracy = 1.0\n",
      "f1 = 1.0\n",
      "Validation scores:\n",
      "accuracy = 0.9208223364356161\n",
      "f1 = 0.6346153846153846\n",
      "\n",
      "Fitting Random Forest ...\n",
      "Training scores:\n",
      "accuracy = 0.9896506216572897\n",
      "f1 = 0.9514973958333334\n",
      "Validation scores:\n",
      "accuracy = 0.9127656618974858\n",
      "f1 = 0.3643724696356275\n",
      "\n",
      "Fitting AdaBoost ...\n",
      "Training scores:\n",
      "accuracy = 0.9439119260957144\n",
      "f1 = 0.7199583839084445\n",
      "Validation scores:\n",
      "accuracy = 0.9409640227809418\n",
      "f1 = 0.7034193998604327\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model_search is defined in modelsearch.py in utils\n",
    "models_d, models_i = model_search(model_list, (train_matrix, y_train), metrics_dict, valid=(valid_matrix, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load models and scores\n",
    "with open('../models/task1-ml/models_dict.pickle','rb') as handle:\n",
    "    models_d = pickle.load(handle)\n",
    "with open('../models/task1-ml/models_info.pickle','rb') as handle:\n",
    "    models_i = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "Models based on decision trees and ensembles show some promise (Decision Trees and Random Forest in particular is overfit).\n",
    "\n",
    "Other models do not do very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at decision tree mode\n",
    "models_d['Decision Trees']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at Random Forest model\n",
    "models_d['Random Forest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid = \\\n",
    "{'bootstrap': [True, False],\n",
    " 'max_depth': list(range(5,50,5)),\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': list(range(1,12)),\n",
    " 'min_samples_split': list(range(2,21)),\n",
    " 'n_estimators': list(range(1,25))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 21.4min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 46.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'bootstrap': [True, False], 'max_depth': [5, 10, 15, 20, 25, 30, 35, 40, 45], 'max_features': ['auto', 'sqrt'], 'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 'n_estimators': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]},\n",
       "          pre_dispatch='2*n_jobs', random_state=0, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rf_random = RandomizedSearchCV(estimator = RandomForestClassifier(), param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=RANDOM_STATE, n_jobs = -1)\n",
    "rf_random.fit(train_matrix, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'max_depth': 45,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 3,\n",
       " 'n_estimators': 17}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting optimized Random Forest ...\n",
      "Training scores:\n",
      "accuracy = 0.9662776967423768\n",
      "f1 = 0.8225187351489672\n",
      "Validation scores:\n",
      "accuracy = 0.9092929573551882\n",
      "f1 = 0.31335436382754994\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = [('optimized Random Forest', str(rf_random.best_params_), RandomForestClassifier().set_params(**rf_random.best_params_))]\n",
    "models, info = model_search(rf, (train_matrix, y_train), metrics_dict, valid=(text_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No performance improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hand-tuning decision tree to reduce variance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_params = {'max_depth':50, 'min_samples_split':3}\n",
    "dt = [('Decision Tree 2', str(rf_params), DecisionTreeClassifier().set_params(**dt_params))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Decision Tree 2 ...\n",
      "Training scores:\n",
      "accuracy = 0.9784330068764326\n",
      "f1 = 0.8933172994330871\n",
      "Validation scores:\n",
      "accuracy = 0.9459647173218503\n",
      "f1 = 0.7307958477508651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models, info = model_search(dt, (text_train, y_train), metrics_dict, valid=(text_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved validation f1 performance"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "hackthenews-task1-a1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
